{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb1facb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw shape: (39194, 56)\n",
      "Clean shape: (39189, 55)\n",
      "Columns: ['price', 'is_free', 'release_year', 'release_month', 'release_day', 'extract_year', 'extract_month', 'extract_day', 'publisherClass_encoded', 'required_age', 'achievements', 'english', 'windows', 'mac', 'linux', 'Single_player', 'Family_Sharing', 'Steam_Achievements', 'Steam_Cloud', 'Full_controller_support'] ...\n",
      "Train size: (31351, 46), Test size: (7838, 46)\n",
      "\n",
      "[Outlier Handling] Winsorization applied to non-binary numeric features:\n",
      " -                    price: cap=(0, 49.99), train clipped=(0+288), test clipped=(0+72)\n",
      " -             release_year: cap=(2020, 2023), train clipped=(0+0), test clipped=(0+0)\n",
      " -            release_month: cap=(1, 12), train clipped=(0+0), test clipped=(0+0)\n",
      " -              release_day: cap=(1, 31), train clipped=(0+0), test clipped=(0+0)\n",
      " -   publisherClass_encoded: cap=(0, 2), train clipped=(0+185), test clipped=(0+56)\n",
      " -             achievements: cap=(0, 100), train clipped=(0+161), test clipped=(0+44)\n",
      " -       days_since_release: cap=(430, 1855), train clipped=(286+310), test clipped=(61+79)\n",
      "\n",
      "[Outlier Handling] Safety constraint:\n",
      " - days_since_release clipped to >= 0 (post-clip negatives still in train=0, test=0)\n",
      "Training MultiOutput LightGBM...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003644 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 561\n",
      "[LightGBM] [Info] Number of data points in the train set: 31351, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 6.493952\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003556 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 561\n",
      "[LightGBM] [Info] Number of data points in the train set: 31351, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 6.406776\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004119 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 561\n",
      "[LightGBM] [Info] Number of data points in the train set: 31351, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 6.057794\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 561\n",
      "[LightGBM] [Info] Number of data points in the train set: 31351, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 6.031960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "d:\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LGBM MultiOutput] Per-target metrics:\n",
      "     target         RMSE           MAE       R2\n",
      "    owners 6.308407e+05  63396.039463 0.208309\n",
      "   players 5.177119e+05  46641.951505 0.159307\n",
      "copiesSold 4.483062e+05  38804.663454 0.203197\n",
      "   revenue 5.330994e+06 400082.532757 0.122953\n",
      "Average R²: 0.17344145709763836\n",
      "\n",
      "Training MultiOutput XGBoost...\n",
      "\n",
      "[XGB MultiOutput] Per-target metrics:\n",
      "     target         RMSE           MAE        R2\n",
      "    owners 7.280718e+05  68686.819886 -0.054545\n",
      "   players 6.501451e+05  51417.816363 -0.325811\n",
      "copiesSold 4.855512e+05  42526.455183  0.065301\n",
      "   revenue 1.319714e+07 540021.658753 -4.374846\n",
      "Average R²: -1.172474886861489\n",
      "\n",
      "Training MultiOutput CatBoost...\n",
      "\n",
      "[CatBoost MultiOutput] Per-target metrics:\n",
      "     target         RMSE           MAE       R2\n",
      "    owners 6.441577e+05  65410.169095 0.174531\n",
      "   players 5.298309e+05  47916.877716 0.119487\n",
      "copiesSold 4.685685e+05  40185.384975 0.129543\n",
      "   revenue 5.055254e+06 393965.284621 0.211335\n",
      "Average R²: 0.15872391124976656\n",
      "\n",
      "[Per-Target] Training per algorithm, per target...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003629 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 561\n",
      "[LightGBM] [Info] Number of data points in the train set: 31351, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 6.493952\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 561\n",
      "[LightGBM] [Info] Number of data points in the train set: 31351, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 6.406776\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 561\n",
      "[LightGBM] [Info] Number of data points in the train set: 31351, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 6.057794\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003096 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 561\n",
      "[LightGBM] [Info] Number of data points in the train set: 31351, number of used features: 42\n",
      "[LightGBM] [Info] Start training from score 6.031960\n",
      "\n",
      "Ensemble weights per target:\n",
      "  owners: {'lgb': 0.33015597519292544, 'xgb': 0.3336768736279973, 'cb': 0.3361671511790773}\n",
      "  players: {'lgb': 0.3344997535004167, 'xgb': 0.31765663075316725, 'cb': 0.3478436157464161}\n",
      "  copiesSold: {'lgb': 0.3450436998589386, 'xgb': 0.31435452381510687, 'cb': 0.3406017763259545}\n",
      "  revenue: {'lgb': 0.2590177762982521, 'xgb': 0.302170315957413, 'cb': 0.43881190774433476}\n",
      "\n",
      "[Ensemble] Per-target metrics:\n",
      "     target         RMSE           MAE        R2\n",
      "    owners 6.647993e+05  66190.003444  0.120780\n",
      "   players 5.531886e+05  48643.164918  0.040141\n",
      "copiesSold 4.710043e+05  40727.886975  0.120469\n",
      "   revenue 6.414907e+06 432921.648802 -0.269952\n",
      "[Ensemble] Average R²: 0.002859446789239356\n",
      "\n",
      "[LGB Per-Target] metrics:\n",
      "     target         RMSE           MAE        R2\n",
      "    owners 6.785522e+05  65386.192843  0.084026\n",
      "   players 5.573337e+05  48056.465660  0.025703\n",
      "copiesSold 4.607897e+05  39872.358331  0.158204\n",
      "   revenue 8.742066e+06 470230.601792 -1.358494\n",
      "[LGB Per-Target] Average R²: -0.2726404702732717\n",
      "\n",
      "[XGB Per-Target] metrics:\n",
      "     target         RMSE           MAE        R2\n",
      "    owners 6.713922e+05  67842.126411  0.103255\n",
      "   players 5.868852e+05  50454.191018 -0.080357\n",
      "copiesSold 5.057748e+05  42930.656553 -0.014182\n",
      "   revenue 7.493624e+06 464266.551681 -0.732967\n",
      "[XGB Per-Target] Average R²: -0.1810628902651953\n",
      "\n",
      "[CB Per-Target] metrics:\n",
      "     target         RMSE           MAE       R2\n",
      "    owners 6.664186e+05  66618.141544 0.116491\n",
      "   players 5.359535e+05  48250.743877 0.099020\n",
      "copiesSold 4.667991e+05  40265.168369 0.136104\n",
      "   revenue 5.160185e+06 400022.250879 0.178255\n",
      "[CB Per-Target] Average R²: 0.13246759655702986\n",
      "\n",
      "Saved ensemble artifacts to: ./artifacts_ensemble\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "# Multi-Output Targets with Per-Target Ensembling\n",
    "# ---------------------------------------------------------------------\n",
    "# Imports\n",
    "import os, re, json, sys\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "import joblib\n",
    "import tabulate\n",
    "\n",
    "# Optional plotting\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    HAS_MPL = True\n",
    "except Exception:\n",
    "    HAS_MPL = False\n",
    "\n",
    "# 3rd-party regressors\n",
    "import lightgbm as lgb\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "except ImportError as e:\n",
    "    raise RuntimeError(\"xgboost is not installed. Run: pip install xgboost\") from e\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "except ImportError as e:\n",
    "    raise RuntimeError(\"catboost is not installed. Run: pip install catboost\") from e\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Config\n",
    "# ---------------------------------------------------------------------\n",
    "DATA_PATH   = \"games_2020_to_2023_6.csv\"\n",
    "ARTIFACT_DIR = \"./artifacts_ensemble\"\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_COLS = ['owners', 'players', 'copiesSold', 'revenue']\n",
    "POST_RELEASE = ['wishlists', 'avgPlaytime', 'followers', 'reviews', 'reviewScore']\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utilities\n",
    "# ---------------------------------------------------------------------\n",
    "def sanitize_column_names(columns):\n",
    "    return [re.sub(r'[^A-Za-z0-9_]+', '_', str(c)) for c in columns]\n",
    "\n",
    "def build_days_since_release(df):\n",
    "    needed = {'release_year','release_month','release_day','extract_year','extract_month','extract_day'}\n",
    "    if not needed.issubset(df.columns):\n",
    "        raise ValueError(f\"Missing date part columns: {needed - set(df.columns)}\")\n",
    "    rel = pd.to_datetime(dict(\n",
    "        year=df['release_year'].astype(int),\n",
    "        month=df['release_month'].astype(int),\n",
    "        day=df['release_day'].astype(int)\n",
    "    ), errors='coerce')\n",
    "    ext = pd.to_datetime(dict(\n",
    "        year=df['extract_year'].astype(int),\n",
    "        month=df['extract_month'].astype(int),\n",
    "        day=df['extract_day'].astype(int)\n",
    "    ), errors='coerce')\n",
    "    df['days_since_release'] = (ext - rel).dt.days\n",
    "    return df\n",
    "\n",
    "def evaluate_predictions(y_true, y_pred, targets):\n",
    "    rows = []\n",
    "    for i, t in enumerate(targets):\n",
    "        rmse = float(np.sqrt(mean_squared_error(y_true[:, i], y_pred[:, i])))\n",
    "        mae  = float(mean_absolute_error(y_true[:, i], y_pred[:, i]))\n",
    "        r2   = float(r2_score(y_true[:, i], y_pred[:, i]))\n",
    "        rows.append({\"target\": t, \"RMSE\": rmse, \"MAE\": mae, \"R2\": r2})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def rmse(true, pred):\n",
    "    return float(np.sqrt(np.mean((true - pred) ** 2)))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Load & Clean\n",
    "# ---------------------------------------------------------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Raw shape:\", df.shape)\n",
    "\n",
    "# Drop obvious non-numeric id/text if present\n",
    "for c in [\"steamid\", \"name\"]:\n",
    "    if c in df.columns:\n",
    "        df.drop(columns=[c], inplace=True, errors=\"ignore\")\n",
    "\n",
    "df = build_days_since_release(df)\n",
    "\n",
    "if \"required_age\" in df.columns:\n",
    "    df[\"required_age\"] = df[\"required_age\"].astype(float)\n",
    "\n",
    "# Convert booleans to ints\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == bool:\n",
    "        df[col] = df[col].astype(int)\n",
    "\n",
    "df.columns = sanitize_column_names(df.columns)\n",
    "\n",
    "# Keep rows with all targets\n",
    "df = df.dropna(subset=TARGET_COLS).copy()\n",
    "# Drop rows where all targets are 0\n",
    "df = df[~((df[TARGET_COLS] == 0).all(axis=1))].copy()\n",
    "# Drop remaining NaNs\n",
    "df = df.dropna().copy()\n",
    "\n",
    "print(\"Clean shape:\", df.shape)\n",
    "print(\"Columns:\", list(df.columns)[:20], \"...\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Train/Test Split with Leakage-Safe X\n",
    "# ---------------------------------------------------------------------\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "X = df[numeric_cols].drop(\n",
    "    columns=[c for c in TARGET_COLS if c in numeric_cols] +\n",
    "            [c for c in POST_RELEASE if c in numeric_cols],\n",
    "    errors=\"ignore\",\n",
    ")\n",
    "y = df[TARGET_COLS].copy()\n",
    "\n",
    "X = X.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "y = y.loc[X.index]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Log-transform targets (helps heavy tails)\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log  = np.log1p(y_test)\n",
    "\n",
    "print(f\"Train size: {X_train.shape}, Test size: {X_test.shape}\")\n",
    "\n",
    "# Save feature order for inference\n",
    "features_used = X_train.columns.tolist()\n",
    "joblib.dump(np.array(features_used), Path(ARTIFACT_DIR) / \"features_used.pkl\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Outlier handling: Feature winsorization (train-fit, apply to both)\n",
    "# ---------------------------------------------------------------------\n",
    "def is_binary(col: pd.Series) -> bool:\n",
    "    u = pd.unique(col.dropna())\n",
    "    if len(u) <= 2:\n",
    "        s = set(u.tolist())\n",
    "        return s.issubset({0, 1})\n",
    "    return False\n",
    "\n",
    "def compute_caps(df: pd.DataFrame, lower_q=0.01, upper_q=0.99):\n",
    "    \"\"\"Return per-column (lo, hi) quantile caps for non-binary numeric columns.\"\"\"\n",
    "    caps = {}\n",
    "    for c in df.columns:\n",
    "        if np.issubdtype(df[c].dtype, np.number) and not is_binary(df[c]):\n",
    "            lo = df[c].quantile(lower_q)\n",
    "            hi = df[c].quantile(upper_q)\n",
    "            if pd.notnull(lo) and pd.notnull(hi) and lo < hi:\n",
    "                caps[c] = (float(lo), float(hi))\n",
    "    return caps\n",
    "\n",
    "def apply_caps(df: pd.DataFrame, caps: dict) -> pd.DataFrame:\n",
    "    df2 = df.copy()\n",
    "    for c, (lo, hi) in caps.items():\n",
    "        if c in df2.columns:\n",
    "            df2[c] = df2[c].clip(lower=lo, upper=hi)\n",
    "    return df2\n",
    "\n",
    "# Keep originals for reporting\n",
    "_Xtr_before = X_train.copy()\n",
    "_Xte_before = X_test.copy()\n",
    "\n",
    "# Fit caps on TRAIN only\n",
    "feature_caps = compute_caps(_Xtr_before, lower_q=0.01, upper_q=0.99)\n",
    "\n",
    "# Count how many would be clipped BEFORE applying\n",
    "clip_report = {}\n",
    "for c, (lo, hi) in feature_caps.items():\n",
    "    if c in _Xtr_before:\n",
    "        tr_low  = (_Xtr_before[c] < lo).sum()\n",
    "        tr_high = (_Xtr_before[c] > hi).sum()\n",
    "    else:\n",
    "        tr_low = tr_high = 0\n",
    "    if c in _Xte_before:\n",
    "        te_low  = (_Xte_before[c] < lo).sum()\n",
    "        te_high = (_Xte_before[c] > hi).sum()\n",
    "    else:\n",
    "        te_low = te_high = 0\n",
    "    clip_report[c] = (lo, hi, int(tr_low), int(tr_high), int(te_low), int(te_high))\n",
    "\n",
    "# Apply caps\n",
    "X_train = apply_caps(X_train, feature_caps)\n",
    "X_test  = apply_caps(X_test,  feature_caps)\n",
    "\n",
    "# Logically bounded safety constraint\n",
    "neg_train_dsr = neg_test_dsr = 0\n",
    "if 'days_since_release' in X_train.columns:\n",
    "    neg_train_dsr = (X_train['days_since_release'] < 0).sum()\n",
    "    neg_test_dsr  = (X_test['days_since_release']  < 0).sum()\n",
    "    X_train['days_since_release'] = X_train['days_since_release'].clip(lower=0)\n",
    "    X_test['days_since_release']  = X_test['days_since_release'].clip(lower=0)\n",
    "\n",
    "# ===== Inspect outlier handling results =====\n",
    "print(\"\\n[Outlier Handling] Winsorization applied to non-binary numeric features:\")\n",
    "for c, (lo, hi, tr_low, tr_high, te_low, te_high) in clip_report.items():\n",
    "    print(f\" - {c:>24s}: cap=({lo:.6g}, {hi:.6g}), \"\n",
    "          f\"train clipped=({tr_low}+{tr_high}), test clipped=({te_low}+{te_high})\")\n",
    "\n",
    "if 'days_since_release' in X_train.columns:\n",
    "    print(\"\\n[Outlier Handling] Safety constraint:\")\n",
    "    print(f\" - days_since_release clipped to >= 0 \"\n",
    "          f\"(post-clip negatives still in train={int(neg_train_dsr)}, test={int(neg_test_dsr)})\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Baseline MultiOutput Models (kept for reference/compat)\n",
    "# ---------------------------------------------------------------------\n",
    "# LightGBM MultiOutput\n",
    "lgb_params = dict(\n",
    "    random_state=RANDOM_STATE, n_jobs=-1,\n",
    "    n_estimators=600, learning_rate=0.02,\n",
    "    num_leaves=63, max_depth=-1, subsample=0.8, colsample_bytree=0.8,\n",
    ")\n",
    "lgb_base = lgb.LGBMRegressor(**lgb_params)\n",
    "lgb_model = MultiOutputRegressor(lgb_base)\n",
    "print(\"Training MultiOutput LightGBM...\")\n",
    "lgb_model.fit(X_train.values, y_train_log.values)\n",
    "\n",
    "y_pred_log = lgb_model.predict(X_test.values)\n",
    "y_pred     = np.expm1(y_pred_log)\n",
    "y_true     = np.expm1(y_test_log.values)\n",
    "lgb_metrics = evaluate_predictions(y_true, y_pred, TARGET_COLS)\n",
    "print(\"\\n[LGBM MultiOutput] Per-target metrics:\\n\", lgb_metrics.to_string(index=False))\n",
    "print(\"Average R²:\", lgb_metrics[\"R2\"].mean())\n",
    "joblib.dump(lgb_model, os.path.join(ARTIFACT_DIR, \"lgb_model.pkl\"))\n",
    "joblib.dump({\"feature_order\": features_used, \"lgb_params\": lgb_params},\n",
    "            os.path.join(ARTIFACT_DIR, \"lgb_model_meta.pkl\"))\n",
    "\n",
    "# XGBoost MultiOutput\n",
    "xgb_params = dict(\n",
    "    objective=\"reg:squarederror\", random_state=RANDOM_STATE,\n",
    "    n_estimators=800, learning_rate=0.03, max_depth=6,\n",
    "    subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, reg_alpha=0.0,\n",
    "    tree_method=\"hist\", nthread=-1,\n",
    ")\n",
    "xgb_base = xgb.XGBRegressor(**xgb_params)\n",
    "xgb_mo_model = MultiOutputRegressor(xgb_base)\n",
    "print(\"\\nTraining MultiOutput XGBoost...\")\n",
    "xgb_mo_model.fit(X_train.values, y_train_log.values)\n",
    "xgb_pred_log = xgb_mo_model.predict(X_test.values)\n",
    "xgb_pred     = np.expm1(xgb_pred_log)\n",
    "xgb_true     = np.expm1(y_test_log.values)\n",
    "xgb_mo_metrics = evaluate_predictions(xgb_true, xgb_pred, TARGET_COLS)\n",
    "print(\"\\n[XGB MultiOutput] Per-target metrics:\\n\", xgb_mo_metrics.to_string(index=False))\n",
    "print(\"Average R²:\", xgb_mo_metrics[\"R2\"].mean())\n",
    "joblib.dump(xgb_mo_model, os.path.join(ARTIFACT_DIR, \"xgb_model.pkl\"))\n",
    "joblib.dump({\"xgb_params\": xgb_params}, os.path.join(ARTIFACT_DIR, \"xgb_params_base.pkl\"))\n",
    "\n",
    "# CatBoost MultiOutput\n",
    "cb_params = dict(\n",
    "    loss_function=\"RMSE\", random_seed=RANDOM_STATE,\n",
    "    n_estimators=1200, learning_rate=0.03, depth=8, l2_leaf_reg=3.0,\n",
    "    subsample=0.8, verbose=0, thread_count=-1\n",
    ")\n",
    "cb_base = CatBoostRegressor(**cb_params)\n",
    "cb_mo_model = MultiOutputRegressor(cb_base)\n",
    "print(\"\\nTraining MultiOutput CatBoost...\")\n",
    "cb_mo_model.fit(X_train.values, y_train_log.values)\n",
    "cb_pred_log = cb_mo_model.predict(X_test.values)\n",
    "cb_pred     = np.expm1(cb_pred_log)\n",
    "cb_true     = np.expm1(y_test_log.values)\n",
    "cb_mo_metrics = evaluate_predictions(cb_true, cb_pred, TARGET_COLS)\n",
    "print(\"\\n[CatBoost MultiOutput] Per-target metrics:\\n\", cb_mo_metrics.to_string(index=False))\n",
    "print(\"Average R²:\", cb_mo_metrics[\"R2\"].mean())\n",
    "joblib.dump(cb_mo_model, os.path.join(ARTIFACT_DIR, \"cb_model.pkl\"))\n",
    "joblib.dump({\"cb_params\": cb_params}, os.path.join(ARTIFACT_DIR, \"catboost_params_base.pkl\"))\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Per-Target Models + Weighted Ensembling\n",
    "# ---------------------------------------------------------------------\n",
    "def make_lgb():\n",
    "    return lgb.LGBMRegressor(\n",
    "        random_state=RANDOM_STATE, n_estimators=800, learning_rate=0.03,\n",
    "        num_leaves=63, subsample=0.8, colsample_bytree=0.8, n_jobs=-1\n",
    "    )\n",
    "\n",
    "def make_xgb():\n",
    "    return xgb.XGBRegressor(\n",
    "        objective=\"reg:squarederror\", random_state=RANDOM_STATE,\n",
    "        n_estimators=900, learning_rate=0.03, max_depth=7,\n",
    "        subsample=0.8, colsample_bytree=0.8, reg_lambda=1.0, reg_alpha=0.0,\n",
    "        tree_method=\"hist\", nthread=-1\n",
    "    )\n",
    "\n",
    "def make_cb():\n",
    "    return CatBoostRegressor(\n",
    "        loss_function=\"RMSE\", random_seed=RANDOM_STATE,\n",
    "        n_estimators=1200, learning_rate=0.03, depth=8, l2_leaf_reg=3.0,\n",
    "        subsample=0.8, rsm=0.8, verbose=0, thread_count=-1\n",
    "    )\n",
    "\n",
    "ALGORITHMS = {\"lgb\": make_lgb, \"xgb\": make_xgb, \"cb\": make_cb}\n",
    "\n",
    "# Train per-target estimators and collect validation predictions\n",
    "per_target_models   = {alg: {} for alg in ALGORITHMS}     # per_target_models[\"cb\"][\"revenue\"] = est\n",
    "per_target_valpreds = {alg: {} for alg in ALGORITHMS}     # store log-space preds on X_test for each target\n",
    "\n",
    "print(\"\\n[Per-Target] Training per algorithm, per target...\")\n",
    "for alg_name, factory in ALGORITHMS.items():\n",
    "    for target in TARGET_COLS:\n",
    "        est = factory()\n",
    "        est.fit(X_train, y_train_log[target].values)\n",
    "        per_target_models[alg_name][target] = est\n",
    "        per_target_valpreds[alg_name][target] = est.predict(X_test)  # log-space\n",
    "\n",
    "# Compute target-wise ensemble weights from inverse RMSE (normal space)\n",
    "ensemble_weights = {}  # {target: {'lgb': w, 'xgb': w, 'cb': w}}\n",
    "for target in TARGET_COLS:\n",
    "    y_true_normal = np.expm1(y_test_log[target].values)\n",
    "    rmses = {}\n",
    "    for alg_name in ALGORITHMS.keys():\n",
    "        yhat_normal = np.expm1(per_target_valpreds[alg_name][target])\n",
    "        rmses[alg_name] = rmse(y_true_normal, yhat_normal)\n",
    "\n",
    "    inv = {alg: (1.0 / rmses[alg]) if rmses[alg] > 0 else 0.0 for alg in rmses}\n",
    "    s = sum(inv.values()) or 1.0\n",
    "    ensemble_weights[target] = {alg: inv[alg] / s for alg in inv}\n",
    "\n",
    "print(\"\\nEnsemble weights per target:\")\n",
    "for t, w in ensemble_weights.items():\n",
    "    print(f\"  {t}: {w}\")\n",
    "\n",
    "# Evaluate ensemble on holdout\n",
    "y_pred_ens = np.zeros((X_test.shape[0], len(TARGET_COLS)), dtype=float)\n",
    "for t_idx, target in enumerate(TARGET_COLS):\n",
    "    weights = ensemble_weights[target]\n",
    "    blend = np.zeros(X_test.shape[0], dtype=float)\n",
    "    for alg_name in ALGORITHMS.keys():\n",
    "        yhat = np.expm1(per_target_valpreds[alg_name][target])  # normal space\n",
    "        blend += weights[alg_name] * yhat\n",
    "    y_pred_ens[:, t_idx] = blend\n",
    "\n",
    "y_true_ens = np.column_stack([np.expm1(y_test_log[t].values) for t in TARGET_COLS])\n",
    "ens_metrics = evaluate_predictions(y_true_ens, y_pred_ens, TARGET_COLS)\n",
    "print(\"\\n[Ensemble] Per-target metrics:\\n\", ens_metrics.to_string(index=False))\n",
    "print(\"[Ensemble] Average R²:\", ens_metrics[\"R2\"].mean())\n",
    "\n",
    "# Also evaluate each per-target algorithm as a “set” for comparison\n",
    "def eval_per_target_family(per_target_models_family):\n",
    "    preds = np.column_stack([\n",
    "        np.expm1(per_target_models_family[t].predict(X_test)) for t in TARGET_COLS\n",
    "    ])\n",
    "    true  = y_true_ens\n",
    "    return evaluate_predictions(true, preds, TARGET_COLS)\n",
    "\n",
    "for alg in ALGORITHMS.keys():\n",
    "    fam_metrics = eval_per_target_family(per_target_models[alg])\n",
    "    print(f\"\\n[{alg.upper()} Per-Target] metrics:\\n\", fam_metrics.to_string(index=False))\n",
    "    print(f\"[{alg.upper()} Per-Target] Average R²:\", fam_metrics[\"R2\"].mean())\n",
    "\n",
    "# Save artifacts\n",
    "joblib.dump(per_target_models,   os.path.join(ARTIFACT_DIR, \"per_target_models.pkl\"))\n",
    "joblib.dump(ensemble_weights,    os.path.join(ARTIFACT_DIR, \"ensemble_weights.pkl\"))\n",
    "joblib.dump(TARGET_COLS,         os.path.join(ARTIFACT_DIR, \"targets.pkl\"))\n",
    "joblib.dump(features_used,       os.path.join(ARTIFACT_DIR, \"feature_order.pkl\"))\n",
    "\n",
    "print(\"\\nSaved ensemble artifacts to:\", ARTIFACT_DIR)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Inference helpers (for API use)\n",
    "# ---------------------------------------------------------------------\n",
    "def _build_input_df(user_input: dict, features_used: list) -> pd.DataFrame:\n",
    "    \"\"\"Build 1-row DF in the exact training feature order (zeros by default).\"\"\"\n",
    "    input_data = {f: 0 for f in features_used}\n",
    "\n",
    "    # Minimal example mapping\n",
    "    # Basics\n",
    "    input_data['price']          = user_input.get('price', 0)\n",
    "    input_data['is_free']        = int(user_input.get('is_free', False))\n",
    "    input_data['required_age']   = user_input.get('required_age', 0)\n",
    "    input_data['achievements']   = user_input.get('achievements', 0)\n",
    "    input_data['english']        = int(user_input.get('english', True))\n",
    "\n",
    "    # Platforms / tags / genres\n",
    "    platform_flags = ['windows', 'mac', 'linux']\n",
    "    tag_flags = [\n",
    "        'Single-player','Family Sharing','Steam Achievements','Steam Cloud',\n",
    "        'Full controller support','Multi-player','Partial Controller Support',\n",
    "        'Steam Trading Cards','PvP','Co-op','Steam Leaderboards','Remote Play Together',\n",
    "        'Online PvP','Shared/Split Screen','Tracked Controller Support','VR Only',\n",
    "        'Shared/Split Screen PvP','Online Co-op','Stats','Shared/Split Screen Co-op'\n",
    "    ]\n",
    "    genre_flags = ['Indie','Casual','Adventure','Action','Simulation',\n",
    "                   'Strategy','RPG','Free To Play','Sports','Racing']\n",
    "\n",
    "    for flag in platform_flags + tag_flags + genre_flags:\n",
    "        if flag in input_data:\n",
    "            input_data[flag] = int(user_input.get(flag, False))\n",
    "\n",
    "    # Encoded publisher\n",
    "    if 'publisherClass_encoded' in input_data:\n",
    "        input_data['publisherClass_encoded'] = user_input.get('publisherClass_encoded', 0)\n",
    "\n",
    "    # Days since release from provided dates\n",
    "    if 'days_since_release' in input_data and 'release_date' in user_input and 'extract_date' in user_input:\n",
    "        release_date = pd.to_datetime(user_input['release_date'])\n",
    "        extract_date = pd.to_datetime(user_input['extract_date'])\n",
    "        input_data['days_since_release'] = (extract_date - release_date).days\n",
    "\n",
    "    return pd.DataFrame([input_data])[features_used]\n",
    "\n",
    "def predict_game_success_single(user_input: dict, per_target_family: dict, features_used: list):\n",
    "    \"\"\"\n",
    "    Predict using a single algorithm's per-target models dict.\n",
    "    per_target_family: dict like {'owners': est, 'players': est, ...}\n",
    "    \"\"\"\n",
    "    X1 = _build_input_df(user_input, features_used)\n",
    "    out_vals = []\n",
    "    for t in TARGET_COLS:\n",
    "        yhat_log = float(per_target_family[t].predict(X1)[0])\n",
    "        out_vals.append(float(np.expm1(yhat_log)))\n",
    "    return {\n",
    "        'owners': int(round(out_vals[0])),\n",
    "        'players': int(round(out_vals[1])),\n",
    "        'copiesSold': int(round(out_vals[2])),\n",
    "        'revenue': float(out_vals[3]),\n",
    "    }\n",
    "\n",
    "def predict_game_success_ensemble(user_input: dict, per_target_models: dict, ensemble_weights: dict, features_used: list):\n",
    "    \"\"\"\n",
    "    Weighted ensemble in NORMAL space per target using inverse-RMSE weights.\n",
    "    per_target_models: {'lgb': {target: est}, 'xgb': {...}, 'cb': {...}}\n",
    "    ensemble_weights: {target: {'lgb': w, 'xgb': w, 'cb': w}}\n",
    "    \"\"\"\n",
    "    X1 = _build_input_df(user_input, features_used)\n",
    "    out = {}\n",
    "    for t in TARGET_COLS:\n",
    "        blend = 0.0\n",
    "        w = ensemble_weights[t]\n",
    "        for alg_name, fam in per_target_models.items():\n",
    "            est = fam[t]\n",
    "            yhat_log = float(est.predict(X1)[0])\n",
    "            yhat = float(np.expm1(yhat_log))\n",
    "            blend += w[alg_name] * yhat\n",
    "        if t == \"revenue\":\n",
    "            out[t] = float(blend)\n",
    "        else:\n",
    "            out[t] = int(round(blend))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "176c6339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aimy\\AppData\\Local\\Temp\\ipykernel_8040\\1869262774.py:56: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_demo = pd.read_csv(\"steam_dataset.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LGB per-target: {'owners': 442705, 'players': 407731, 'copiesSold': 366512, 'revenue': 12116192.385623004}\n",
      "XGB per-target: {'owners': 244657, 'players': 209119, 'copiesSold': 82370, 'revenue': 3825492.177672367}\n",
      "CB  per-target: {'owners': 673210, 'players': 422103, 'copiesSold': 592582, 'revenue': 9122949.247156745}\n",
      "Ensemble      : {'owners': 454109, 'players': 349639, 'copiesSold': 354191, 'revenue': 8297518.15234535}\n",
      "\n",
      "[Ensemble] Example vs Actuals\n",
      "+------------+---------------+---------------+------------------+--------------------+------------+\n",
      "| Metric     | Predicted     | Actual        | Absolute Error   | Percentage Error   | Accuracy   |\n",
      "+============+===============+===============+==================+====================+============+\n",
      "| owners     | 454,109       | 436,490.0     | 17,619           | 4.04%              | 95.96%     |\n",
      "+------------+---------------+---------------+------------------+--------------------+------------+\n",
      "| players    | 349,639       | 436,490.0     | 86,851           | 19.90%             | 80.10%     |\n",
      "+------------+---------------+---------------+------------------+--------------------+------------+\n",
      "| copiesSold | 354,191       | 306,548.0     | 47,643           | 15.54%             | 84.46%     |\n",
      "+------------+---------------+---------------+------------------+--------------------+------------+\n",
      "| revenue    | $8,297,518.15 | $8,401,853.00 | $104,334.85      | 1.24%              | 98.76%     |\n",
      "+------------+---------------+---------------+------------------+--------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Demo on a specific game row\n",
    "# ---------------------------------------------------------------------\n",
    "def get_game_input_format(df, steamid=None, row_num=None):\n",
    "    if steamid is not None:\n",
    "        game = df[df[\"steamid\"] == steamid]\n",
    "        if game.empty:\n",
    "            raise ValueError(f\"No game found with steamid {steamid}\")\n",
    "        game = game.iloc[0]\n",
    "    elif row_num is not None:\n",
    "        game = df.iloc[row_num]\n",
    "    else:\n",
    "        raise ValueError(\"You must provide either a steamid or a row_num.\")\n",
    "\n",
    "    release_date = f\"{int(game['release_year'])}-{int(game['release_month']):02d}-{int(game['release_day']):02d}\"\n",
    "    extract_date = f\"{int(game['extract_year'])}-{int(game['extract_month']):02d}-{int(game['extract_day']):02d}\"\n",
    "\n",
    "    input_data = {\n",
    "        \"price\": float(game.get(\"price\", 0)),\n",
    "        \"is_free\": bool(game.get(\"is_free\", False)),\n",
    "        \"required_age\": int(game.get(\"required_age\", 0)),\n",
    "        \"achievements\": int(game.get(\"achievements\", 0)),\n",
    "        \"english\": bool(game.get(\"english\", True)),\n",
    "        \"windows\": bool(game.get(\"windows\", True)),\n",
    "        \"mac\": bool(game.get(\"mac\", False)),\n",
    "        \"linux\": bool(game.get(\"linux\", False)),\n",
    "        \"release_date\": release_date,\n",
    "        \"extract_date\": extract_date,\n",
    "        \"publisherClass_encoded\": int(game.get(\"publisherClass_encoded\", 0)),\n",
    "    }\n",
    "\n",
    "    # Add boolean flags if present\n",
    "    boolean_cols = game.index[\n",
    "        game.index.isin([\n",
    "            \"Single-player\",\"Family Sharing\",\"Steam Achievements\",\"Steam Cloud\",\n",
    "            \"Full controller support\",\"Multi-player\",\"Partial Controller Support\",\n",
    "            \"Steam Trading Cards\",\"PvP\",\"Co-op\",\"Steam Leaderboards\",\"Remote Play Together\",\n",
    "            \"Online PvP\",\"Shared/Split Screen\",\"Tracked Controller Support\",\"VR Only\",\n",
    "            \"Shared/Split Screen PvP\",\"Online Co-op\",\"Stats\",\"Shared/Split Screen Co-op\",\n",
    "            \"Indie\",\"Casual\",\"Adventure\",\"Action\",\"Simulation\",\"Strategy\",\"RPG\",\n",
    "            \"Free To Play\",\"Sports\",\"Racing\"\n",
    "        ])\n",
    "    ]\n",
    "    for col in boolean_cols:\n",
    "        input_data[col] = bool(game[col])\n",
    "\n",
    "    # Extract targets for comparison\n",
    "    players = game.get(\"players\", np.nan)\n",
    "    owners = game.get(\"owners\", np.nan)\n",
    "    copies_sold = game.get(\"copiesSold\", np.nan)\n",
    "    revenue = game.get(\"revenue\", np.nan)\n",
    "\n",
    "    return (input_data, players, owners, copies_sold, revenue)\n",
    "\n",
    "# Demo (uncomment to run a quick check)\n",
    "df_demo = pd.read_csv(\"steam_dataset.csv\")\n",
    "steamid = 315210\n",
    "#315210\n",
    "#2124490\n",
    "#235520\n",
    "#1302990\n",
    "\n",
    "#340020\n",
    "#290100\n",
    "#24880\n",
    "#333640\n",
    "#282880\n",
    "#251570\n",
    "\n",
    "\n",
    "(\n",
    "    game_dict, players, owners, copies_sold, revenue\n",
    ") = get_game_input_format(df_demo, steamid=steamid)\n",
    "\n",
    "# Single families\n",
    "preds_lgb = predict_game_success_single(game_dict, per_target_models[\"lgb\"], features_used)\n",
    "preds_xgb = predict_game_success_single(game_dict, per_target_models[\"xgb\"], features_used)\n",
    "preds_cb  = predict_game_success_single(game_dict, per_target_models[\"cb\"],  features_used)\n",
    "# Ensemble\n",
    "preds_ens = predict_game_success_ensemble(game_dict, per_target_models, ensemble_weights, features_used)\n",
    "\n",
    "print(\"\\nLGB per-target:\", preds_lgb)\n",
    "print(\"XGB per-target:\", preds_xgb)\n",
    "print(\"CB  per-target:\", preds_cb)\n",
    "print(\"Ensemble      :\", preds_ens)\n",
    "\n",
    "# Optional: compare one set to actuals\n",
    "actual = {\"owners\": owners, \"players\": players, \"copiesSold\": copies_sold, \"revenue\": revenue}\n",
    "headers = [\"Metric\", \"Predicted\", \"Actual\", \"Absolute Error\", \"Percentage Error\", \"Accuracy\"]\n",
    "rows = []\n",
    "def append_rows(preds):\n",
    "    for key in actual:\n",
    "        pred_val = preds[key]\n",
    "        actual_val = actual[key]\n",
    "        abs_error = abs(actual_val - pred_val) if pd.notna(actual_val) else np.nan\n",
    "        pct_error = (abs_error / actual_val * 100) if (pd.notna(actual_val) and actual_val != 0) else np.nan\n",
    "        accuracy = 100 - pct_error if pd.notna(pct_error) else np.nan\n",
    "        if key == \"revenue\":\n",
    "            row = [key, f\"${pred_val:,.2f}\", f\"${actual_val:,.2f}\", f\"${abs_error:,.2f}\" if pd.notna(abs_error) else \"N/A\",\n",
    "                   f\"{pct_error:.2f}%\" if pd.notna(pct_error) else \"N/A\", f\"{accuracy:.2f}%\" if pd.notna(accuracy) else \"N/A\"]\n",
    "        else:\n",
    "            row = [key, f\"{pred_val:,}\", f\"{actual_val:,}\", f\"{int(abs_error):,}\" if pd.notna(abs_error) else \"N/A\",\n",
    "                   f\"{pct_error:.2f}%\" if pd.notna(pct_error) else \"N/A\", f\"{accuracy:.2f}%\" if pd.notna(accuracy) else \"N/A\"]\n",
    "        rows.append(row)\n",
    "\n",
    "append_rows(preds_ens)\n",
    "print(\"\\n[Ensemble] Example vs Actuals\")\n",
    "print(tabulate.tabulate(rows, headers=headers, tablefmt=\"grid\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
